{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일 입출력 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import distutils.dir_util\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def write_json(data, fname):\n",
    "    def _conv(o):\n",
    "        if isinstance(o, np.int64) or isinstance(o, np.int32):\n",
    "            return int(o)\n",
    "        raise TypeError\n",
    "\n",
    "    parent = os.path.dirname(fname)\n",
    "    distutils.dir_util.mkpath(\"./\" + parent)\n",
    "    with io.open(\"./\" + fname, \"w\", encoding=\"utf8\") as f:\n",
    "        json_str = json.dumps(data, ensure_ascii=False, default=_conv)\n",
    "        f.write(json_str)\n",
    "\n",
    "\n",
    "def load_json(fname):\n",
    "    with open(fname, encoding='utf8') as f:\n",
    "        json_obj = json.load(f)\n",
    "\n",
    "    return json_obj\n",
    "\n",
    "\n",
    "def debug_json(r):\n",
    "    print(json.dumps(r, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as spr\n",
    "from collections import Counter\n",
    "from implicit.evaluation import  *\n",
    "from implicit.als import AlternatingLeastSquares as ALS\n",
    "from scipy.sparse import hstack, vstack\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "song_meta = pd.read_json(\"./../file/song_meta.json\")\n",
    "# train = pd.read_json(\"./../file/train_pos.json\")\n",
    "# test = pd.read_json(\"./../file/validation_pos.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../file/train_pos.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('../file/validation_pos.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['all_word'] = train.tags + train.genre_kor + train.plyst_title_pos\n",
    "test['all_word'] = test.tags + test.genre_kor + test.plyst_title_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = train.all_word + test.all_word\n",
    "tt = tt.dropna()\n",
    "tt = list(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(tt, size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wv_mean(word_list):\n",
    "    wv_mean = []\n",
    "    for w in word_list:\n",
    "        try:\n",
    "            wv_mean.append(np.array(w2v.wv[w]))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    if not wv_mean:\n",
    "        return []\n",
    "    return np.array(wv_mean).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vectored'] = train.all_word.apply(lambda e: get_wv_mean(e))\n",
    "test['vectored'] = test.all_word.apply(lambda e: get_wv_mean(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['istrain'] = 1\n",
    "test['istrain'] = 0\n",
    "\n",
    "n_train = len(train)\n",
    "n_test = len(test)\n",
    "\n",
    "# train + test\n",
    "plylst = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# playlist id\n",
    "plylst[\"nid\"] = range(n_train + n_test)\n",
    "\n",
    "# id <-> nid\n",
    "plylst_id_nid = dict(zip(plylst[\"id\"],plylst[\"nid\"]))\n",
    "plylst_nid_id = dict(zip(plylst[\"nid\"],plylst[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plylst_tag = plylst['tags']\n",
    "tag_counter = Counter([tg for tgs in plylst_tag for tg in tgs])\n",
    "tag_dict = {x: tag_counter[x] for x in tag_counter}\n",
    "\n",
    "tag_id_tid = dict()\n",
    "tag_tid_id = dict()\n",
    "for i, t in enumerate(tag_dict):\n",
    "    tag_id_tid[t] = i\n",
    "    tag_tid_id[i] = t\n",
    "\n",
    "n_tags = len(tag_dict)\n",
    "\n",
    "plylst_song = plylst['songs']\n",
    "song_counter = Counter([sg for sgs in plylst_song for sg in sgs])\n",
    "song_dict = {x: song_counter[x] for x in song_counter}\n",
    "\n",
    "song_id_sid = dict()\n",
    "song_sid_id = dict()\n",
    "for i, t in enumerate(song_dict):\n",
    "    song_id_sid[t] = i\n",
    "    song_sid_id[i] = t\n",
    "\n",
    "n_songs = len(song_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [-0.13327028, -0.04235815, -0.24068263, -0.233...\n",
       "1         [-0.08894731, -0.003893503, -0.0747035, 0.0303...\n",
       "2         [-0.07311141, -0.124093056, 0.17151015, 0.0105...\n",
       "3         [-0.0033125186, -0.0040749004, 0.12479437, 0.1...\n",
       "4         [-0.094459996, 0.2342832, -0.044105448, 0.1128...\n",
       "                                ...                        \n",
       "138081    [-0.0699301, -0.07824149, 0.11491039, 0.140681...\n",
       "138082    [-0.15912956, -0.0077224365, 0.14708532, 0.190...\n",
       "138083    [-0.0027779818, -0.10349687, -0.2909028, -0.38...\n",
       "138084    [0.06729251, -0.12507215, 0.035921358, -0.0133...\n",
       "138085    [-0.20647204, -0.004208231, 0.1137497, 0.06362...\n",
       "Length: 138086, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138086, 6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plylst_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\long8v\\Anaconda3\\envs\\long36v\\lib\\site-packages\\pandas\\core\\indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\long8v\\Anaconda3\\envs\\long36v\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "plylst['songs_id'] = plylst['songs'].map(lambda x: [song_id_sid.get(s) for s in x if song_id_sid.get(s) != None])\n",
    "plylst['tags_id'] = plylst['tags'].map(lambda x: [tag_id_tid.get(t) for t in x if tag_id_tid.get(t) != None])\n",
    "plylst['w2v'] = plylst['vectored']\n",
    "plylst_use = plylst[['istrain','nid','updt_date','songs_id','tags_id']]\n",
    "\n",
    "# 곡의 개수와 태그의 개수를 할당\n",
    "plylst_use.loc[:,'num_songs'] = plylst_use['songs_id'].map(len)\n",
    "plylst_use.loc[:,'num_tags'] = plylst_use['tags_id'].map(len)\n",
    "plylst_use = plylst_use.set_index('nid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "plylst_use['vectored'] = pd.Series(list(train.vectored) + list(test.vectored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "plylst_train = plylst_use.iloc[:n_train,:]\n",
    "plylst_test = plylst_use.iloc[n_train:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csr (compressed sparse row matrix 생성)\n",
    "row = np.repeat(range(n_train), plylst_train['num_songs'])\n",
    "col = [song for songs in plylst_train['songs_id'] for song in songs]\n",
    "dat = np.repeat(1, plylst_train['num_songs'].sum())\n",
    "train_songs_A = spr.csr_matrix((dat, (row, col)), shape=(n_train, n_songs))\n",
    "\n",
    "row = np.repeat(range(n_train), plylst_train['num_tags'])\n",
    "col = [tag for tags in plylst_train['tags_id'] for tag in tags]\n",
    "dat = np.repeat(1, plylst_train['num_tags'].sum())\n",
    "train_tags_A = spr.csr_matrix((dat, (row, col)), shape=(n_train, n_tags))\n",
    "\n",
    "# csr (compressed sparse row matrix 생성)\n",
    "row = np.repeat(range(n_test), plylst_test['num_songs'])\n",
    "col = [song for songs in plylst_test['songs_id'] for song in songs]\n",
    "dat = np.repeat(1, plylst_test['num_songs'].sum())\n",
    "test_songs_A = spr.csr_matrix((dat, (row, col)), shape=(n_test, n_songs))\n",
    "\n",
    "row = np.repeat(range(n_test), plylst_test['num_tags'])\n",
    "col = [tag for tags in plylst_test['tags_id'] for tag in tags]\n",
    "dat = np.repeat(1, plylst_test['num_tags'].sum())\n",
    "test_tags_A = spr.csr_matrix((dat, (row, col)), shape=(n_test, n_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\long8v\\Anaconda3\\envs\\long36v\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\long8v\\Anaconda3\\envs\\long36v\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "plylst_train['vectored'] = plylst_train.vectored.apply(lambda e: e if len(e) else np.random.random(500))\n",
    "plylst_test['vectored'] = plylst_test.vectored.apply(lambda e: e if len(e) else np.random.random(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "row, column, and data array must all be the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-ee133f5cc49c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msong\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msongs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplylst_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'songs_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msong\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msongs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplylst_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vectored'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtest_songs_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_songs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplylst_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\long36v\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     55\u001b[0m                     \u001b[1;31m# (data, ij) format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                     \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\long36v\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\long36v\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_native\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'row index exceeds matrix dimensions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\long36v\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mnnz\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mcount_nonzero\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mnon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mzero\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\long36v\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mgetnnz\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mnnz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 raise ValueError('row, column, and data array must all be the '\n\u001b[0m\u001b[0;32m    246\u001b[0m                                  'same length')\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: row, column, and data array must all be the same length"
     ]
    }
   ],
   "source": [
    "for i, wv_trn in enumerate(w2v_train):\n",
    "    vectored_train[i].extend(wv_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_feature_csr = hstack([train_songs_A, train_tags_A, train_w2v_a])\n",
    "test_merged_feature_csr  = hstack([test_songs_A, test_tags_A, test_w2v_a])\n",
    "\n",
    "train_songs_A_T = train_songs_A.T.tocsr()\n",
    "train_tags_A_T  = train_tags_A.T.tocsr()\n",
    "\n",
    "whole_datasets_csr       = vstack([train_merged_feature_csr, test_merged_feature_csr])\n",
    "whole_datasets_csr_T     = whole_datasets_csr.T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668533, 138086)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_datasets_csr_T.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec(pids):\n",
    "    res = []\n",
    "\n",
    "    for pid in tqdm(pids):\n",
    "        p = np.zeros((n_songs,1))\n",
    "        p[plylst_test.loc[pid,'songs_id']] = 1\n",
    "\n",
    "        val = train_songs_A.dot(p).reshape(-1)\n",
    "\n",
    "        songs_already = plylst_test.loc[pid, \"songs_id\"]\n",
    "        tags_already = plylst_test.loc[pid, \"tags_id\"]\n",
    "\n",
    "        cand_song = train_songs_A_T.dot(val)\n",
    "        cand_song_idx = cand_song.reshape(-1).argsort()[-250:][::-1]\n",
    "\n",
    "        cand_song_idx = cand_song_idx[np.isin(cand_song_idx, songs_already) == False][:100]\n",
    "        rec_song_idx = [song_sid_id[i] for i in cand_song_idx]\n",
    "\n",
    "        cand_tag = train_tags_A_T.dot(val)\n",
    "        cand_tag_idx = cand_tag.reshape(-1).argsort()[-15:][::-1]\n",
    "\n",
    "        cand_tag_idx = cand_tag_idx[np.isin(cand_tag_idx, tags_already) == False][:10]\n",
    "        rec_tag_idx = [tag_tid_id[i] for i in cand_tag_idx]\n",
    "\n",
    "        res.append({\n",
    "            \"id\": plylst_nid_id[pid],\n",
    "            \"songs\": rec_song_idx,\n",
    "            \"tags\": rec_tag_idx\n",
    "        })\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ee09ad39434b749a133213c843a3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23015.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = rec(plylst_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_factor = 128\n",
    "# n_epoch  = 15.0\n",
    "\n",
    "# als_model = ALS(factors=n_factor, regularization=0.08)\n",
    "# als_model.fit(whole_datasets_csr_T * n_epoch)\n",
    "\n",
    "# song_model = ALS(use_gpu=False)\n",
    "# tag_model = ALS(use_gpu=False)\n",
    "# song_model.user_factors = als_model.user_factors\n",
    "# tag_model.user_factors = als_model.user_factors\n",
    "\n",
    "# song_model.item_factors = als_model.item_factors[:n_songs]\n",
    "# tag_model.item_factors  = als_model.item_factors[n_songs:]\n",
    "\n",
    "# print(song_model.item_factors.shape)\n",
    "# print(tag_model.item_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = []\n",
    "\n",
    "\n",
    "\n",
    "# for u in tqdm(range(test_merged_feature_csr.shape[0])):\n",
    "#     song_rec = song_model.recommend(u, test_songs_A, N=100)\n",
    "#     song_rec = [song_sid_id[x[0]] for x in song_rec]\n",
    "    \n",
    "#     tag_rec = tag_model.recommend(u, test_tags_A, N=10)\n",
    "#     tag_rec = [tag_tid_id[x[0]] for x in tag_rec]\n",
    "    \n",
    "#     res.append({\n",
    "#             # train test의 vconcat 된 matrix에서 id를 추출하는 것이기 때문에 n_train 이후부터 test 데이터 셋임\n",
    "#             # 따라서 u + n_train이 각각의 test 셋에서의 id\n",
    "#             \"id\": plylst_nid_id[u + n_train],\n",
    "#             \"songs\": song_rec,\n",
    "#             \"tags\": tag_rec\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "write_json(res, \"results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
